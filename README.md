# ğŸ§Š ELT Pipeline: API + MySQL to S3 â†’ Snowflake â†’ dbt

A modern ELT pipeline that:
- Extracts data from a public API (CoinGecko) and optionally from MySQL
- Loads the raw data into Amazon S3 as cost-optimized Parquet files
- Ingests data into Snowflake using `COPY INTO`
- Transforms data inside Snowflake using `dbt`
- Schedules the pipeline using Apache Airflow

---

## ğŸ“Œ Features

- âœ… **ELT architecture**: fast, cloud-optimized data pipeline  
- ğŸª™ **CoinGecko API integration**: sample extract of real-time crypto prices  
- ğŸ’¾ **S3 Storage**: compressed Parquet format to minimize cost  
- â„ï¸ **Snowflake DWH**: scalable cloud-based analytics platform  
- ğŸ” **Airflow DAG**: automates daily scheduling  
- ğŸ“Š **dbt models**: modular SQL transformations for analytics  

---

## ğŸ“ Project Structure

```
elt_pipeline/
â”‚
â”œâ”€â”€ dags/                         # Airflow DAG for scheduling
â”‚   â””â”€â”€ elt_crypto_pipeline.py
â”œâ”€â”€ extract/                      # Extraction scripts
â”‚   â””â”€â”€ extract_coingecko.py
â”œâ”€â”€ load/                         # Load scripts to S3
â”‚   â””â”€â”€ load_to_s3.py
â”œâ”€â”€ ingest/                       # Ingest data from S3 to Snowflake
â”‚   â””â”€â”€ snowflake_ingest.py
â”œâ”€â”€ dbt_project/                  # dbt transformations
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ transform_crypto.sql
â””â”€â”€ requirements.txt              # Python dependencies
```

---

## âš™ï¸ Setup Instructions

### 1. ğŸ”§ Environment Setup

Install Python dependencies:

```bash
pip install -r requirements.txt
```

Install and start Apache Airflow (example for local environment):

```bash
export AIRFLOW_HOME=~/airflow
airflow db init
airflow webserver --port 8080
airflow scheduler
```

### 2. ğŸ” Configuration

Set your credentials for:

- AWS S3 (bucket, access keys, region)
- Snowflake (user, password, account, integration name)
- MySQL (optional)

Update the following scripts with your credentials:
- `load_to_s3.py`
- `elt_crypto_pipeline.py`

### 3. ğŸ›°ï¸ Run the Pipeline

In Airflow UI:
- Enable the `elt_crypto_pipeline` DAG
- Trigger manually or wait for scheduled run

---

## ğŸ§ª Optional: Run dbt Transformations

```bash
cd dbt_project
dbt init
dbt run
```

---

## ğŸ’° S3 Cost Optimization

- Files are saved in **Parquet format** with **Snappy compression**
- Organize S3 keys by date prefix to enable partition pruning
- Optionally set **S3 lifecycle rules** to transition old data to Glacier

---

## ğŸš€ Future Extensions

- Add **MySQL** data extraction module
- Implement **Great Expectations** for data quality checks
- Add **Superset or Looker** for BI dashboards
- Use **AWS Lambda** + **Snowpipe** for real-time ingestion

---

## ğŸ§‘â€ğŸ’» Author

Maurice Oboya  
[GitHub](https://github.com/mauriceoboya)

---

## ğŸ“„ License

This project is licensed under the MIT License.
